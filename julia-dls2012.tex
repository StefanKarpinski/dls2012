\documentclass[9pt]{sigplanconf}

% VBS: Change back to 9pt for paper submission. 12 pages max.

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

\conferenceinfo{Dynamic Languages Symposium 2012}{October 22, 2012, Tucson, USA.} 
\copyrightyear{2012} 
%\copyrightdata{[to be supplied]} 

%\titlebanner{Julia}        % These are ignored unless
\preprintfooter{Julia: A fresh approach to technical computing}   % 'preprint' option specified.

\title{Julia: A fresh approach to technical computing}
%\subtitle{Subtitle Text, if any}

\authorinfo{Jeff Bezanson}
           {MIT}
           {jeff.bezanson@gmail.com}
\authorinfo{Stefan Karpinski}
           {MIT}
           {stefan@karpinski.org}
\authorinfo{Viral B. Shah}
           {}
           {viral@mayin.org}
\authorinfo{Alan Edelman}
           {MIT}
           {edelman@math.mit.edu}

\maketitle
\begin{abstract}
  Dynamic programming languages have become popular for scientific
  computing. They are generally considered highly productive, but
  lacking in performance. This paper presents Julia, a new dynamic
  language for technical computing, designed for performance from the
  beginning by adapting and extending modern programming language
  techniques. A design based on generic functions and a rich type
  system simultaneously enables an expressive programming model and
  successful type inference, leading to good performance for a wide
  range of programs. This makes it possible for much of Julia's
  library to be written in Julia itself, while also incorporating
  best-of-breed C and Fortran libraries.
\end{abstract}

\category{D.3.2}{Programming Languages}{Very high-level languages}

\terms
Programming Language, Technical computing, Scientific computing, High
performance computing

\keywords
Programming Language, Technical computing, Scientific computing, High
performance computing

\section{Introduction}

Convenience is winning. Despite continued advances in compiler technology
and execution frameworks for high-performance computing, programmers
routinely use high-level dynamic languages for algorithm
development in applied math, engineering,  and the sciences. These systems
(prominent examples include Python \cite{numpy}, R \cite{Rlang},
MATLAB\textregistered, Octave \cite{Octave}, and SciLab \cite{scilab})
have greatly increased
productivity, but are known to lack performance for many demanding applications.
The result is a two-tiered software world, where
C and FORTRAN are used for key libraries and production code, while
high-level languages are used for interaction and scripting overall workflow.
A new approach to dynamic language design can
change this situation, providing productivity and performance at once. We
should embrace the emerging preference for ``scripting'' style languages,
and ask how these systems can better provide for the future of technical
computing.

The ``two-tier'' architecture, for example, writing an application in
Python with performance-critical code written in C, seems like a good
way to balance performance and productivity. However, there are many
reasons to move away from this architecture \footnote{To be clear,
  applications can and often do consist of code in many languages.  It
  is perfectly reasonable of course to call existing well understood,
  accurate, and well performing libraries.  Working codes adhere to
  the old saying, ``If it ain't broke, don't fix it''. It is also
  reasonable is to pick several languages, based on the user's sense
  of a good expressive match to the application.  The problem we are
  exploring is the common switch to another language made only for
  reasons of performance.}.  Naturally, it would be preferable to
write compute-intensive code in a more productive language as well,
especially when developing parallel algorithms, where code complexity
can increase dramatically.  Programming in two languages can also be
more complex than using either language by itself, due to interfacing
issues such as converting between type domains and handling memory
reclamation.  These interfacing issues may also add overhead when
calling between layers. When such a system is used for mathematical
programming there is pressure to write ``vectorized'' code, which is
not natural for every problem. Lastly, from a compiler's point of
view, these designs make it difficult to perform whole-program
optimization. It is difficult to perform domain-specific optimizations
of C code, and expressing algorithms at a higher level makes certain
optimizations easier.

Fortunately, there has been significant progress in improving the
performance of dynamic languages. Projects like the Python compiler
framework PyPy \cite{pypyjit} have been fairly successful. Similar efforts
exist for languages from LISP onward. The common feature of all
such projects is that they seek to add performance to an existing
language. This is obviously useful, but we are somewhat surprised to find
it has not led to the desired situation outlined above. Julia is designed
for performance from the beginning, and we feel this
seemingly-subtle difference turns out to be crucial.

%In our design, the compiler machinery that provides performance
%is also available for extra expressivity in programs.

``Built-in'' performance means that the compiler's type machinery is also
available within the language, adding expressiveness. This, in turn,
allows more functionality to be implemented in
libraries. Many of the key differences between languages used by different
disciplines (e.g. R for statistics) could be expressed in libraries, but
are instead either part of the language core, or implemented in C where
they are more difficult to modify or extend. When optimizers for these
languages are developed, knowledge of key library functions often must be
encoded into the compiler. Even Common LISP, for which there are several
highly-optimizing compilers, specifies arithmetic in the language, and
yet users do not all agree on how arithmetic should behave. Some
users require specialized types such as fixed-point numbers or intervals,
or support for ``missing data'' values as in R.

Julia has the potential to solve this problem by providing infrastructure
that can be shared across domains, without sacrificing the ease and
immediacy of current popular systems.
We take advantage of, and validate, this infrastructure by writing Julia's
standard library in the language itself, which (1) makes the code more
generic and increases our productivity, (2) allows inlining library
code into user code and vice-versa, and (3) enables direct type analysis
of the library instead of requiring knowledge of library functions to
be built in to the compiler. New users are able to read the standard
library code, and modify it or imitate it for their own purposes.
%Ultimately, performance is about flexibility, not just getting answers
%faster.

%% Ultimately, performance is about more than getting
%% an answer faster; it is about expressivity and flexibility. Our core
%% strategy for achieving this is to employ a sophisticated type system that can
%% nevertheless be ignored by users who aren't interested in it. The type
%% system becomes, in a sense, an optional tool for library writers.

%% often languages start with a performance or parallelization goal and work
%% from there. we start from the opposite direction, designing for maximum
%% flexibility and ease-of-use, and betting that this power can be leveraged to
%% meet increasingly ambitious performance goals. One area where a flexible
%% high-level language can potentially help performance is custom code
%% generation. Often hand-written C code is impractical or insufficient for
%% obtaining the highest performance. Julia's type inference and JIT compiler
%% make it easier to generate efficient code at a more abstract, symbolic level.

Many of the ideas explored here are not exclusively applicable to technical
computing, but we have chosen to target that application area for several
reasons. First, technical computing has unique concerns that can be
especially awkward or inefficient to handle in existing dynamic languages.
Examples include the need for a wide variety of numeric types, and the need for
efficient arrays of those types. Second, the performance
of high-level technical computing languages has begun to seriously lag behind
that of more mainstream languages (notably JavaScript), creating a present
need for attempts to improve the situation.
General-purpose languages like Java or perhaps even JavaScript could
be used for technical computing, but we feel the community will continue to
prefer environments that cater to its syntactic needs, and are able to
prioritize issues of numerical accuracy and performance.

%% my parallelism principles:
%% 1. People won't use a language *just* to get parallelism. They will
%% live with their favorite language's parallel extensions (ipython,
%% cilk, PCT, etc.)
%% 2. Making parallelism implicit in evaluation semantics is not the way
%% to get effective parallelism. I think I heard Arvind say they tried it
%% with parallel Haskell for 10 years, and it didn't work.
%% 3. If a language isn't as easy as matlab or R, people will keep using
%% matlab or R.

\section{The Essence of Julia}

Julia's primary means of abstraction is dynamic multiple dispatch.
Much of a language consists of mechanisms for selecting
code to run in different situations --- from method selection to
instruction selection. We use only dynamic multiple dispatch for this
purpose, which is possible through sufficiently expressive
dispatch rules. To add usability to this flexibility,
types can generally be ignored when not used to specify dispatch behavior.

Types may optionally be used to make declarations, which are considered by
the compiler and checked at run time when necessary. However, we do not
require declarations for performance. To achieve this, Julia's compiler
automatically specializes methods for types encountered at run time
(or at compile time, to the extent types are known then). Effectively,
every method is a template (in the C++ sense) by default, with
parameterization and instantiation directed by the compiler. We feel this
design is in line with a general trend towards automation in compiler
and language design.

%designing for type inference gives us two things
%- can design library with conscious tradeoffs
%- avoid premature optimization

\section{Language Design}


Static typing appears to have many advantages from an objective, theoretical
standpoint: earlier error detection, generally better performance, and
support for more accurate tools are often cited in this context.
Nevertheless, developers are ``voting with their code'' for languages that
lack strong static typing disciplines. This is the phenomenon that Julia's
design addresses, and as such we must present our view of the advantages of
dynamic languages. In particular, we do not assume that \emph{every}
feature of these languages is equally important.
We hypothesize that the following forms of ``dynamism'' are
the most useful:

\begin{itemize}
\item The ability to run code at load time and compile time, eliminating
some of the distractions of build systems and configuration files.
\item A universal {\tt Any} type as the only true static type,
allowing the issue of static types to be ignored when desired.
\item Never rejecting code that is syntactically well-formed.
\item Behavior that depends only on run-time types (unlike, for example, C++,
where virtual methods are dispatched by run-time type and function overloads
are dispatched by static type).
\end{itemize}

We explicitly forgo the following features in the interest of preserving
the possibility of static typing in a reasonably broad category of
situations:

\begin{itemize}
\item Types themselves are immutable.
\item The type of a value cannot change over its lifetime.
\item Local variable environments are not reified.
\item Program code is immutable, but new code may be generated and executed at any time.
\item Not all bindings are mutable ({\tt const} identifiers are allowed).
\end{itemize}

This set of restrictions allows the compiler to see all uses of local
variables, and perform dataflow analysis on local variables using only
local information. This is important, since it allows user code to call
statically-unknown functions without interfering with optimizations done
around such call sites. Statically-unknown function calls arise in
many contexts, such as calling a function taken from an untyped data structure,
or dynamically dispatching a method call due to unknown argument types.

The core Julia language contains the following components:

\begin{enumerate}
\item A syntax layer, to translate surface syntax to a suitable
intermediate representation (IR).
\item A symbolic language and corresponding data structures for representing
certain kinds of types, and implementations of lattice operators ($meet$,
$join$, and $\leq$) for those types.
\item An implementation of generic functions and dynamic multiple dispatch
based on those types.
\item Compiler intrinsic functions for accessing the object model
(type definition, method definition, object allocation, element access,
testing object identity, and accessing type tags).
\item Compiler intrinsic functions for native arithmetic, bit string operations,
and calling native (C or FORTRAN) functions.
\item A mechanism for binding top-level names.
\end{enumerate}

The IR describes a function body as a sequence of assignment operations,
function calls, labels, and conditional branches. Julia's semantics
are those of a standard imperative language: statements are executed in order,
with function arguments evaluated eagerly. All values are conceptually
references, and are passed by reference as in LISP.

Julia's core evaluation semantics are particularly bland, because all of the
interesting work has been moved to the generic function system. Every
function definition is actually a definition of a method for some generic
function for some combination of argument types. The ``feel'' of the language
derives mostly from the fact that every function call is dynamically
dispatched to the most specific matching method definition, based on the
types of all arguments.


\subsection{Types}

Julia uses dynamic typing, which means that the universal type {\tt Any}
is the only static type. Our design philosophy is that types should be
quite powerful and expressive, but nearly invisible to the user. Julia
programmers must be able to ignore the type system completely if they do
not wish to make explicit use of its functionality.

Julia treats types as symbolic descriptions of sets of values. Every value has
a unique, immutable, run-time implementation type. Objects carry type tags, and
types themselves are Julia objects that can be manipulated at run time.
Julia has five kinds of types:

\begin{enumerate}
\item abstract types, which may have explicitly-declared subtypes and supertypes
\item composite types (similar to C structs), which have named fields and
      explicitly-declared supertypes
\item bits types, whose values are represented as bit strings, and which may
      have explicitly-declared supertypes
\item tuples, immutable ordered collections of values. The type of a tuple is
defined recursively as a tuple of the types of the elements. Tuple types are
covariant in their element types. A tuple is used to represent the type of a
method's arguments.
\item union types, abstract types constructed from other types via set union
\end{enumerate}

Abstract types, composite types, and bits types may have parameters, which
makes it possible to express variants of a given type (for example, array types
with different element types). These types are all invariant with respect to
their parameters (i.e. two versions of the same type with different parameters
are simply different, and have no subtype or supertype relationship). Type
constructors are applied using curly braces, as in {\tt Array\{Float64,1\}}
(the {\tt Array} type is parameterized by element type and rank).

Bits types allow users to add new fixed-width number-like types and obtain the
same performance that primitive numeric types enjoy in other systems. Julia's
``built in'' numeric types are defined as bits types. Julia method dispatch
is based on types rather than field lookup, so whether a value is of a bits
type or composite type is a representation detail that is generally
invisible.

As an extra complexity, tuple types may end in a special {\tt ...} type that
indicates any number of elements may be added. This is used to express the
types of variadic methods. For example the type {\tt (String, Int...)}
indicates a tuple where the first element is a String and any number of
trailing integers may be present.

Union types are used primarily to construct tight least upper bounds
when the inference algorithm needs to join unrelated types. For example,
a method might return an {\tt Int} or a {\tt String} in separate
arms of a conditional. In this case its type can be inferred as
{\tt Union(Int,String)}. Union types are also useful for defining
ad-hoc type hierarchies different from those imagined when the types
involved were first defined. For example, we could use
{\tt Union(Int,Range{Int})} as the type of array indexes, even though
there is no {\tt Index} type inherited by both constituents. Lastly,
union types can be used to declare methods applicable to multiple types.

The key to the utility of Julia's type system is its implementation of two
important functions: the subtype predicate, which determines whether one type
is a subset of another, and type intersection, which computes a type that is a
subtype of two given types. These functions form the basis of method dispatch
logic and type inference.

\subsection{Notational Conveniences}

An important goal is for users to be able to write Julia programs with
virtually no knowledge of type system details. Therefore we allow writing
parametric types without parameters, or omitting trailing parameters.
{\tt Array} refers to any kind of dense array, {\tt Array\{Float64\}} refers
to a Float64 Array of any rank, and {\tt Array\{Float64,2\}} refers to a
2-dimensional Float64 Array.

This design also makes it easy to add parameters to types later; existing
code does not need to be modified.

\subsection{Standard Type Hierarchy}
Here we present an excerpt from the standard library, showing how a few
important types are defined. The fields of composite types are redacted for
the sake of brevity. The {\tt <:} syntax indicates a declared subtype
relation.

\begin{verbatim}
abstract Type{T}
type AbstractKind  <: Type; end
type BitsKind      <: Type; end
type CompositeKind <: Type; end
type UnionKind     <: Type; end

abstract Number
abstract Real     <: Number
abstract Float    <: Real
abstract Integer  <: Real
abstract Signed   <: Integer
abstract Unsigned <: Integer

bitstype 32 Float32 <: Float
bitstype 64 Float64 <: Float

bitstype 8  Bool <: Integer
bitstype 32 Char <: Integer

bitstype 8  Int8   <: Signed
bitstype 8  Uint8  <: Unsigned
bitstype 16 Int16  <: Signed
bitstype 16 Uint16 <: Unsigned
bitstype 32 Int32  <: Signed
bitstype 32 Uint32 <: Unsigned
bitstype 64 Int64  <: Signed
bitstype 64 Uint64 <: Unsigned

abstract AbstractArray{T,N}
type Array{T,N} <: AbstractArray{T,N}; end
\end{verbatim}

% pattern emerges where ``user code'' has no types, and the more types
% written in code the more ``library-like'' it becomes (cite scala?)


\subsection{Syntax}

Julia has a simple block-structured syntax, with notation for type
and method definition, control flow, and special syntax for important
operators.

\subsection{Method Definition}
Method definitions have a long (multi-line) form and a short form.

\begin{verbatim}
function iszero(x::Number)
    return x==0
end

iszero(x) = (x==0)
\end{verbatim}

A type declaration with {\tt ::} on an argument is a dispatch specification.
When types are omitted, the default is {\tt Any}.
A {\tt ::} expression may be added to any program expression, in which case
it acts as a run-time type assertion. As a special case, when {\tt ::} is
applied to a variable name in statement position (a construct which otherwise
has no effect) it means the variable \emph{always} has the specified type,
and values will be converted to that type (by calling {\tt convert}) on
assignment to the variable.

Note that there is no distinct type context; types are computed by ordinary
expressions
evaluated at run time. For example, {\tt f(x)::Int} is lowered to the
function call {\tt typeassert(f(x),Int)}.

Anonymous functions are written using the syntax {\tt x->x+1}.

Local variables are introduced implicitly by assignment. Modifying a
global variable requires a {\tt global} declaration.

Operators are simply functions with special calling syntax. Their
definitions look the same as those of ordinary functions, for example
{\tt +(x,y)~=~...}, or {\tt function~+(x,y)}.

When the last argument in a method signature is followed by {\tt ...}
the method accepts any number of arguments, and the last argument name
is bound to a tuple containing the tail of the argument list. The syntax
{\tt f(t...)} ``splices'' the contents of an iterable object {\tt t} as the
arguments to {\tt f}.

\subsubsection{Parametric Methods}

It is often useful to refer to parameters of argument types inside methods,
and to specify constraints on those parameters for dispatch purposes.
Method parameters address these needs. These parameters behave a bit like
arguments, but they are always derived automatically from
the argument types and not specified explicitly by the caller.
The following signature presents a typical example:

\begin{verbatim}
function assign{T<:Integer}(a::Array{T,1}, i, n::T)
\end{verbatim}

This signature is applicable to 1-dimensional arrays whose element type is
some kind of integer, any type of second argument, and a third argument
that is the same type as the array's element type. Inside the method,
{\tt T} will be bound to the array element type.

The primary use of this construct is to write methods applicable to a
family of parametric types
(e.g. all integer arrays, or all numeric arrays)
despite invariance. The other use is
writing ``diagonal'' constraints as in the example above. Such diagonal
constraints significantly complicate the type lattice operators.


\subsection{Control Flow}

\begin{verbatim}
if condition1 || (a && b)
    # single line comment
elseif !condition2
else
    # otherwise
end

while condition
    # body
end

for i in range
    # body
end
\end{verbatim}

A {\tt for} loop is translated to a while loop with method calls according
to the iteration interface ({\tt start}, {\tt done}, and {\tt next}):

\begin{verbatim}
state = start(range)
while !done(range, state)
  (i, state) = next(range, state)
  # body
end
\end{verbatim}

This design for iteration was chosen because it is not tied to mutable
heap-allocated state, such as an iterator object that updates itself.

\subsection{Special Operators}

Special syntax is provided for certain functions.

\begin{tabular}{|l|l|}\hline
surface syntax     & lowered form \\\hline \hline
{\tt a[i, j]}      & {\tt ref(a, i, j)} \\\hline
{\tt a[i, j] = x}  & {\tt assign(a, x, i, j)} \\\hline
{\tt [a; b]}       & {\tt vcat(a, b)} \\\hline
{\tt [a, b]}       & {\tt vcat(a, b)} \\\hline
{\tt [a b]}        & {\tt hcat(a, b)} \\\hline
{\tt [a b; c d]}   & {\tt hvcat((2,2), a, b, c, d)}\\\hline
\end{tabular}

\subsection{Type Definition}

\begin{verbatim}
# abstract type
abstract Complex{T<:Real} <: Number

# composite type
type ComplexPair{T<:Real} <: Complex{T}
    re::T
    im::T
end

# bits type
bitstype 128 Complex128 <: Complex{Float64}

# type alias
typealias TwoOf{T} (T,T)
\end{verbatim}

\subsubsection{Constructors}

Composite types are applied as functions to construct instances.
The default constructor accepts values for each field as arguments.
Users may override the default constructor by writing method definitions
with the same name as the type inside the {\tt type} block. Inside the
{\tt type} block the identifier {\tt new} is bound to a pseudofunction
that actually constructs instances from field values. The constructor
for the {\tt Rational} type is a good example:

\begin{verbatim}
type Rational{T<:Integer} <: Real
    num::T
    den::T

    function Rational(num::T, den::T)
        if num == 0 && den == 0
            error("invalid rational: 0//0")
        end
        g = gcd(den, num)
        new(div(num, g), div(den, g))
    end
end
\end{verbatim}

This allows {\tt Rational} to enforce representation as a fraction in
lowest terms.


\subsection{Generic Functions}

% talk about how this is a sneaky way to collect type information. since
% signatures specify dispatch behavior they are not redundant information.

The vast majority of Julia functions (in both the library and user programs)
are generic functions, meaning they contain multiple definitions or methods for
various combinations of argument types. When a generic function is applied,
the most specific definition that matches the run-time argument types is
invoked. Generic functions have appeared in several object systems in the past,
notably CLOS \cite{closoverview} and Dylan \cite{dylanlang}.
Julia is distinguished from these in that
it uses generic functions as its primary abstraction mechanism, putting it in
the company of research languages like Diesel \cite{dieselspec}. Aside
from being practical for highly polymorphic mathematical styles of programming,
as we will discuss, this design is satisfying also because it permits
expression of most of the popular patterns of object-oriented programming,
while leaving the core language with fewer distinct features.

Generic functions are a natural fit for mathematical programming. For example,
consider implementing exponentiation (the {\tt \^{}} operator in Julia).
This function
lends itself to multiple definitions, specializing on both arguments
separately: there might be one definition for two floating-point numbers that
calls a standard math library routine, one definition for the case where the
second argument is an integer, and separate definitions for the case where the
first argument is a matrix. In Julia these signatures would be written as
follows:

\begin{verbatim}
function ^(x::Float64, p::Float64)
function ^(x, p::Int)
function ^(x::Matrix, p)
\end{verbatim}

\subsection{Singleton Kinds}

A generic function's method table is effectively a dictionary where the keys
are types. This suggests that it should be just as easy to define or look up
methods with types themselves as with the types of values. Defining methods on
types directly is analogous to defining class methods in class-based object
systems. With multi-methods, definitions can be associated with combinations
of types, making it easy to represent properties not naturally owned by one
type.

To accomplish this, we introduce a special singleton kind {\tt Type\{T\}},
which contains the type {\tt T} as its only value.
%This is similar to the
%self-type pattern [cite], except along the type-of hierarchy rather than along
%the subtype-of hierarchy.
The result is a feature similar to {\tt eql}
specializers in CLOS, except only for types. An example use is defining
type traits:

\begin{verbatim}
typemax(::Type{Int64}) = 9223372036854775807
\end{verbatim}

This definition will be invoked by the call {\tt typemax(Int64)}. Note that
the name of a method argument can be omitted if it is not referenced.

Types are useful as method arguments in several other cases. One example is
file I/O, where a type can be used to specify what to read. The call
{\tt read(file,Int32)} reads a 4-byte integer and returns it as an {\tt Int32}
(a fact that the type inference process is able to discover). We find this
more elegant and convenient than systems where enums or special constants must
be used for this purpose, or where the type information is implicit
(e.g. through return-type overloading).

\subsection{Method Sorting and Ambiguity}
Methods are stored sorted by specificity, so the first matching method
(as determined by the subtype predicate) is always the correct one to invoke.
This means much of the dispatch logic is contained in the sorting process.
Comparing method signatures for specificity is not trivial. As one might
expect, the ``more specific''\footnote{Actually, ``not less specific'',
since specificity is a partial order.}
predicate is quite similar to the subtype
predicate, since a type that is a subtype of another is indeed more specific
than it. However, a few additional rules are necessary to capture the
intuitive concept of ``more specific''. In fact until this point
``more specific'' has had no formal meaning; its formal definition is
summarized as the disjunction of the following rules ($A$ is more specific
than $B$ if):

\pagebreak

\begin{enumerate}
\item $A$ is a subtype of $B$
\item $A$ is of the form {\tt T\{P\}} and $B$ is of the form {\tt S\{Q\}}, and
$T$ is a subtype of $S$ for some parameter values
\item The intersection of $A$ and $B$ is nonempty, more specific than $B$, and
not equal to $B$, and $B$ is not more specific than $A$
\item $A$ and $B$ are tuple types, $A$ ends in a vararg ({\tt ...}) type,
and $A$ would be more specific than $B$ if its vararg type were expanded to
give it the same number of elements as $B$
\end{enumerate}

Rule 2 means that declared subtypes are always more specific than their
declared supertypes regardless of type parameters. Rule 3 is mostly useful for
union types: if $A$ is {\tt Union(Int32,String)} and $B$ is {\tt Number}, $A$
should
be more specific than $B$ because their intersection ({\tt Int32}) is clearly
more specific than $B$. Rule 4 means that argument types are more important for
specificity than argument count; if $A$ is {\tt (Int32...)} and $B$ is
{\tt (Number, Number)} then $A$ is more specific.

Julia uses \emph{symmetric} multiple dispatch, which means all argument types
are equally important. Therefore, ambiguous signatures are possible.
For example, given {\tt foo(x::Number, y::Int)} and
{\tt foo(x::Int, y::Number)} it is not clear which method to call when both
arguments are integers. We detect ambiguities when a method is added, by
looking for a pair of signatures with a non-empty intersection where neither
one is more specific than the other. A warning message is displayed for each
ambiguity, showing the user the computed type intersection so it is clear what
definition is missing. For example:

\begin{verbatim}
Warning: New definition foo(Int,Number) is ambiguous with foo(Number,Int).
        Make sure foo(Int,Int) is defined first.
\end{verbatim}


\subsection{Intrinsic Functions}

The run-time system contains a small number of primitive functions for tasks
like determining the type of a value, accessing fields of composite types, and
constructing values of each of the supported kinds of concrete types. There are
also arithmetic intrinsics corresponding to machine-level operations like
fixed-width integer addition, bit shifts, etc. These intrinsic functions are
implemented only in the code generator and do not have callable entry points.
They operate on bit strings, which are not first class values but can be
converted to and from Julia bits types via boxing and unboxing operations.

In our implementation, the core system also provides functions for constructing
arrays and accessing array elements. Although this is not strictly necessary,
we did not want to expose unsafe memory operations (e.g. load and store
primitives) in the language. In an earlier implementation, the core system
provided a bounds-checked {\tt Buffer} abstraction, but having both this type
and the user-level {\tt Array} type proved inconvenient and confusing.


\subsection{Design Limitations}

In our design, type information always flows along with values, in the
forward control flow direction. This prevents us from doing certain tricks
that static type systems are capable of, such as return-type overloading.
Return-type overloading requires a robust notion of the type of a value
\emph{context}---the type expected or required of some term---in order to
select code on that basis. There are other cases where ``backwards'' type
flow might be desirable, such as determining the type of a container based
on the type of a value stored into it at a later program point. It may be
possible to get around this limitation in the future using inversion of
control---passing a function argument whose result type has already been
inferred, and using that type to construct a container before elements are
computed.

Modularity is a perennial difficulty with multiple dispatch, as any
function may apply to any type, and there is no point where functions or
types are closed to future definitions. Thus at the moment Julia is
essentially a whole-program compiler. We plan to implement a module system
that will at least allow code to control which name bindings and definitions
it sees. Such modules could be separately compiled to the extent that
programmers are willing to ask for their definitions to be ``closed''.

Lastly, at this time Julia uses a bit more memory than we would prefer.
Our compiler data structures, type information, and generated native code
take up more space than the compact bytecode representations used by many
dynamic languages.

\subsection{Extra Features}

\begin{enumerate}
\item symmetric coroutines
\item macros
\item ccall
\end{enumerate}

\section{Implementation}

\section{Implementation}

\subsection{Method Dispatch}

Much of the implementation is organized around method dispatch. The dispatch
logic is both a large portion of the behavior of Julia functions, and the
entry point of the compiler's type inference and specialization logic.

\subsubsection{Method Caching and Specialization}

The first step of method dispatch is to look for the argument types in a
per-function cache. The cache has an entry for (almost) every set of concrete
types to which the function has been applied. Concrete types are hash-consed,
so they can be compared by simple pointer comparison. This makes cache lookup
faster than the $subtype$ predicate. As part of hash-consing, concrete types
are assigned small integer IDs. The ID of the first argument is used as a
primary key into a method cache, so when signatures differ only in the
type of the first argument a simple indexed lookup suffices.

On a cache miss, a slower search for the matching definition is performed using
$subtype$.
Then, type inference is invoked on the matching method using the types
of the actual arguments. The resulting type-annotated and optimized method is
stored in the cache. In this way, method dispatch is the primary source of type
information for the compiler.

\subsubsection{Method Specialization Heuristics}

Our aggressive use of code specialization has the obvious pitfall that it might
lead to excessive code generation, consuming memory and compile time. We found
that a few mild heuristics suffice to give a usable system with reasonable
resource requirements.

The first order of business is to ensure that the dispatch and specialization
process converges. The reason it might not is that our type inference algorithm
is implemented in Julia itself. Calling a method on a certain type $A$ can cause
the type inference code to call the same method on type $B$, where types
$A$ and $B$
follow an infinite ascending chain in either of two partial orders (the
$typeof$ order or the $subtype$ order). Singleton kinds are the most
prominent example, as type inference might attempt to successively consider
{\tt Int32}, {\tt Type\{Int32\}}, {\tt Type\{Type\{Int32\}\}}, and so on. We
stop this process by replacing any nestings of {\tt Type} with the
unspecialized version of {\tt Type} during method specialization (unless the
original method declaration actually specified a type like
{\tt Type\{Type\{Int32\}\}}).

The next heuristic avoids specializing methods for tuple types of every length.
Tuple types are cached as the intersubsection of the declared type of the method
slot with the generic tuple type {\tt (Any...)}. This makes the resulting cache
entry valid for any tuple argument, again unless the method declaration
contained a more specific tuple type. Note that all of these heuristics require
corresponding changes in the method cache lookup procedure, since they yield
cache entries that do not have to exactly match candidate arguments.

A similar heuristic is applied to variadic methods, where we wish to avoid
caching argument lists of every length. This is done by capping argument lists
at the length of the longest signature of any method in the same generic
function. The ``capping'' involves replacing the last argument with a
{\tt ...} type. Ideally, we want to form the biggest type that's not a
supertype of any other method signatures. However, this is not always possible
and the capped type might conflict with another signature. To deal with this
case, we find all non-empty intersubsections of the capped type with other
signatures, and add dummy cache entries for them. Hitting one of these entries
alerts the system that the arguments under consideration are not really in the
cache. Without the dummy entries, some arguments might incorrectly match the
capped type, causing the wrong method to be invoked.

The next heuristic concerns singleton kinds again. Because of the singleton
kind feature, every distinct type object ({\tt Any}, {\tt Number}, {\tt Int},
etc.) passed to a method might trigger a new specialization. However, most
methods are not ``class methods'' and are not concerned with type objects.
Therefore, if no method definition in a certain function involves {\tt Type}
for a certain argument slot, then that slot is not specialized for different
type objects.

Finally, we introduce a special type {\tt ANY} that can be used in a method
signature to hint that a slot should not be specialized. This is used in the
standard library in a small handful of places, and in practice is less
important than the heuristics described above.


\subsection{Type Inference}

Types of program expressions and variables are inferred by forward
dataflow analysis\footnote{Adding a reverse dataflow pass could potentially
improve type information, but we have not yet done this.}.
A key feature of this form of type inference is that variable types are
inferred at each use, since assignment is allowed to change the type of
a variable.
We determine a maximum fixed-point (MFP) solution using
Algorithm~\ref{alg1}, based on
Mohnen's graph-free dataflow analysis framework \cite{graphfree}. The basic
idea is to keep track of the state (the types of all variables) at each program
point, determine the effect of each statement on the state, and ensure that
type information from each statement eventually propagates to all other
statements reachable by control flow. We augment the basic algorithm with
support for mutually-recursive functions
(functions are treated as program points that might need to be revisited).

The origin of the type information used by the MFP algorithm is
evaluation of known functions over the type domain \cite{abstractinterp}.
This is done by the $eval$ subroutine. The $interpret$ subroutine calls
$eval$, and also handles assignment statements by returning the new types
of affected variables. Each known function
call is either to one of the small number of built-in functions, in which
case the result type is computed by a (usually trivial) hand-written
type transfer function, or to a generic function, in which case the result
type is computed by recursively invoking type inference. In the generic
function case, the inferred argument types are met ($\sqcap$) with the
signatures of each method definition. Matching methods are those where the
meet (greatest lower bound)
is not equal to the bottom type ({\tt None} in Julia).
Type inference is invoked on each matching
method, and the results are joined ($\sqcup$) together. The following equation
summarizes this process:

\[
T(f,t_{arg}) = \bigsqcup_{(s,g) \in f}T(g,t_{arg} \sqcap s)
\]

\noindent
$T$ is the type inference function.
$t_{arg}$ is the inferred argument tuple type. The tuples $(s,g)$
represent the signatures $s$ and their associated definitions $g$ within
generic function $f$.

Two optimizations are helpful here. First, it is rarely
necessary to consider all method definitions. Since methods are stored in
sorted order, as soon as the union of the signatures considered so far is a
supertype of $t_{arg}$, no more definitions need to be considered.
Second, the join operator employs \emph{widening} \cite{widening}:
if a type becomes too large it may simply return {\tt Any}. In this case
the recursive inference process may stop immediately.

% \subsubsection{MFP Dataflow Algorithm}

% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}

% \begin{algorithm}
% \caption{Infer function return type}
% \label{alg1}
% \begin{algorithmic}
% \REQUIRE function $F$, argument type tuple $A$, abstract execution stack $T$
% \ENSURE result type $T.R$
% \STATE $V \leftarrow$ set of all locally-bound names
% \STATE $V_{a} \leftarrow$ argument names
% \STATE $n \leftarrow \length(F)$
% \STATE $W \leftarrow \{1\}$ \COMMENT {set of program counters}
% \STATE $P_r \leftarrow \emptyset$ \COMMENT {statements that recur}
% \STATE $\forall i, S[1,V[i]] \leftarrow \text{Undef}$
% \STATE $\forall i, S[1,V_{a}[i]] \leftarrow A[i]$
% \WHILE{$W \neq \emptyset$}
%  \STATE $p \leftarrow \operatorname{choose}(W)$
%  \REPEAT
%   \STATE $W \leftarrow W - p$
%   \STATE $new \leftarrow interpret(F[p],S[p],T)$
%   \IF {$T.rec$}
%    \STATE $P_r \leftarrow P_r \cup \{p\}$
%    \STATE $T.rec \leftarrow \text{false}$
%   \ENDIF
%   \STATE $p\prime \leftarrow p+1$
%   \IF{$F[p] = $\texttt{(goto l)}}
%    \STATE $p\prime \leftarrow l$
%   \ELSIF{$F[p] = $\texttt{(gotoif cond l)}}
%    \IF {\NOT $new \leq S[l]$}
%     \STATE $W \leftarrow W \cup \{l\}$
%     \STATE $S[l] \leftarrow S[l] \sqcup new$
%    \ENDIF
%   \ELSIF{$F[p] = $\texttt{(return e)}}
%    \STATE $p\prime \leftarrow n+1$
%    \STATE $r \leftarrow eval(e,S[p],T)$
%    \IF {\NOT $r \leq T.R$}
%     \STATE $T.R \leftarrow T.R \sqcup r$
%     \STATE $W \leftarrow W \cup P_r$
%    \ENDIF
%   \ENDIF
%   \IF{$p\prime \leq n$ \AND \NOT $new \leq S[p\prime]$}
%    \STATE $S[p\prime] \leftarrow S[p\prime] \sqcup new$
%    \STATE $p \leftarrow p\prime$
%   \ENDIF
%  \UNTIL{$p\prime = n+1$}
% \ENDWHILE
% \STATE {$T.rec \leftarrow P_r \neq \emptyset$}
% \end{algorithmic}
% \end{algorithm}

\subsubsection{Interprocedural Type Inference}

Type inference is invoked through ``driver'' Algorithm~\ref{alg2}
which manages mutual recursion and memoization of inference results.
A stack of abstract activation records is maintained and used to detect
recursion. Each function has a property $incomplete(F,A)$ indicating that
it needs to be revisited when new information is discovered about the
result types of functions it calls. The $incomplete$ flags collectively
represent a set analogous to $W$ in Algorithm~\ref{alg1}.

The outer loop in Algorithm~\ref{alg2} looks for an existing activation
record for its input function and argument types. If one is found, it
marks all records from that point to the top of the stack, identifying
all functions involved in the call cycle. These marks
are discovered in Algorithm~\ref{alg1} when $interpret$ returns, and all
affected functions are considered $incomplete$. Algorithm~\ref{alg2}
continues to re-run inference on incomplete functions, updating the
inferred result type, until no recursion occurs or the result type
converges.

% \begin{algorithm}
% \caption{Interprocedural type inference}
% \label{alg2}
% \begin{algorithmic}
% \REQUIRE function $F$, argument type tuple $A$, abstract execution stack $S$
% \ENSURE returned result type
% \STATE $R \leftarrow \bot$
% \IF {$\recall(F,A)$ exists}
%  \STATE $R \leftarrow \recall(F,A)$
%  \IF {\NOT $incomplete(F,A)$}
%   \RETURN $R$
%  \ENDIF
% \ENDIF
% \STATE $f \leftarrow S$
% \WHILE {\NOT $\operatorname{empty}(f)$}
%  \IF {$f.F$ is $F$ \AND $f.A=A$}
%   \STATE $r \leftarrow S$
%   \WHILE {\NOT $r=\tail(f)$}
%    \STATE $r.rec \leftarrow \text{true}$
%    \STATE $r \leftarrow \tail(r)$
%   \ENDWHILE
%   \RETURN $f.R$
%  \ENDIF
%  \STATE $f \leftarrow \tail(f)$
% \ENDWHILE
% \STATE $T \leftarrow \extend(S, \Frame(F=F,A=A,R=R,rec=\text{false}))$
% \STATE invoke Algorithm~\ref{alg1} on $F,A,T$
% \STATE $\recall(F,A) \leftarrow T.R$
% \STATE $incomplete(F,A) \leftarrow (T.rec \land \neg(R=T.R))$
% \RETURN $T.R$
% \end{algorithmic}
% \end{algorithm}

Because this algorithm approximates run-time behavior, we are free to
change it without affecting the behavior of user programs --- except
that they might run faster. One valuable improvement is
to attempt full evaluation of branch conditions, and remove branches
with constant conditions.

%\subsubsection{Type Transfer Functions}

%todo ?

\subsection{Lattice Operations}

Our type lattice is complicated by the presence of type parameters, unions,
and diagonal type constraints in method signatures. Fortunately, for our
purposes only the $\leq$ ($subtype$) relation needs to be computed accurately,
as it bears final responsibility for whether a method is applicable to
given arguments. Type union and intersubsection, used to estimate
least upper bounds and greatest lower bounds, respectively, may both be
conservatively approximated. If their results are too coarse, the
worst that can happen is performing method dispatch or type checks
at run time, since the inference process will simply conclude that it does
not know precise types.

A complication arises from the fact that our abstract domain is
available in a first-class fashion to user programs. When a program
contains a type-valued expression, we want to know which type it will
evaluate to, but this is not possible in general. Therefore in addition
to the usual \emph{type imprecision} (not knowing the type of a value),
we must also model \emph{type uncertainty}, where a type itself is
known imprecisely. A common example is application of the {\tt typeof}
primitive to a value of imprecise type. What is the abstract result of
{\tt typeof(x::Number)}? We handle this with a special type kind that
represents a \emph{range} rather than a point within the type lattice.
These kinds are essentially the type variables used in bounded
polymorphism \cite{boundedquant}. In this example, the
transfer function for {\tt typeof} is allowed to return
{\tt Type\{T<:Number\}}, where {\tt T} is a new type variable.


\subsubsection{Subtype Predicate}

See Algorithm~\ref{alg3}. Note that extensional type equality can be
computed as $(A\leq~B\land~B\leq~A)$, and this is used for types in
invariant context (i.e. type parameters). The algorithm uses subroutines
$p(A)$ which gives the parameters of type $A$, and $super(A)$ which gives
the declared supertype of $A$.

% \begin{algorithm}
% \caption{Subtype}
% \label{alg3}
% \begin{algorithmic}
% \REQUIRE types $A$ and $B$
% \ENSURE $A \leq B$
% \IF {$A$ is a tuple type}
%  \IF {$B$ is not a tuple type}
%   \RETURN false
%  \ENDIF
%  \FOR {$i=1$ \TO $\length(A)$}
%   \IF {$A[i]$ is $T...$}
%    \IF {$\last(B)$ exists and is not $S...$}
%     \RETURN false
%    \ENDIF
%    \RETURN $subtype(T,B[j])), i \leq j \leq \length(B)$
%   \ELSIF {$i > \length(B)$ \OR \NOT $subtype(A[i],B[i])$}
%    \RETURN false
%   \ELSIF {$B[i]$ is $T...$}
%    \RETURN $subtype(A[j],T)), i < j \leq \length(A)$
%   \ENDIF
%  \ENDFOR
% \ELSIF {$A$ is a union type}
%  \RETURN $\forall t \in A, subtype(t,B)$
% \ELSIF {$B$ is a union type}
%  \RETURN $\exists t \in B, subtype(A,t)$
% \ENDIF
% \WHILE {$A \neq \texttt{Any}$}
%  \IF {$\typename(A) = \typename(B)$}
%   \RETURN {$subtype(\p(A),\p(B)) \land subtype(\p(B),\p(A))$}
%  \ENDIF
%  \STATE $A \leftarrow \super(A)$
% \ENDWHILE
% \IF {$A$ is of the form {\tt Type\{T\}}}
%  \RETURN $subtype(typeof(\p(A)[1]),B)$
% \ELSIF {$B$ is of the form {\tt Type\{T\}}}
%  \STATE $B \leftarrow \p(B)[1]$
%  \RETURN $subtype(A,B) \land subtype(B,A)$
% \ENDIF
% \RETURN $B = \texttt{Any}$
% \end{algorithmic}
% \end{algorithm}


\subsubsection{Type Union}

Since our type system explicitly supports unions, the union of $T$ and
$S$ can be computed simply by constructing the type {\tt Union(T,S)}.
An obvious simplification is performed: if one of $T$ or $S$ is a
subtype of the other, it can be removed from the union. Nested union
types are flattened, followed by pairwise simplification.

\subsubsection{Type Intersubsection}

This is the difficult one: given types $T$ and $S$, we must try to compute
the smallest type $R$ such that
$\forall s, s \in T \land s \in S \Rightarrow s \in R$.
The conservative solution is to give up on finding the smallest such type, and
return \emph{some} type with this property. Simply returning $T$ or $S$
suffices for correctness, but in practice this algorithm
makes the type inference process nearly useless. A slightly better
algorithm is to check whether one argument is a subtype of the other, and
return the smaller type. It is also possible to determine quickly, in
many cases, that two types are disjoint, and return $\bot$. With these
two enhancements we start to obtain some useful type information. However,
we need to do much better to take full advantage of the framework set up
so far.

Our algorithm has two phases. First, the structures of the two input types
are analyzed in a manner similar to $subtype$, except a constraint
environment is built, with entries $T\leq S$ for type variables $T$ in
covariant contexts (tuples) and entries $T=S$ for type variables $T$ in
invariant contexts (type parameters). In the second phase the constraints
are solved with an algorithm (Algorithm~\ref{alg5}) similar to that
used by traditional polymorphic type systems \cite{MLtypeinf}.

The code for handling tuples and union types is similar to that in
Algorithm~\ref{alg3}, so we focus instead on intersecting types in the
nominative hierarchy (Algorithm~\ref{alg4}). The base case occurs when
the input types are from the same family, i.e. have the same
$typename$. All we need to do is visit each parameter to collect any
needed constraints, and otherwise check that the parameters are equal.
When a parameter is a type variable, it is effectively covariant, and
must be intersected with the corresponding parameter of the other type
to form the final result.

% \begin{algorithm}
% \caption{Intersubsection of nominative types}
% \label{alg4}
% \begin{algorithmic}
% \REQUIRE types $A$ and $B$, current constraint environment
% \ENSURE return $T$ such that $A \sqcap B \leq T$, updated environment
% \IF {$typename(A) = typename(B)$}
%  \STATE $pa \leftarrow \operatorname{copy}(\p(A))$
%  \FOR {$i=1$ \TO $\length(\p(A))$ }
%   \IF {$\p(A)[i]$ is a typevar}
%    \STATE {add $(\p(A)[i]=\p(B)[i])$ to constraints}
%   \ELSIF {$\p(B)[i]$ is a typevar}
%    \STATE {add $(\p(B)[i]=\p(A)[i])$ to constraints}
%   \ENDIF
%   \STATE $pa[i] \leftarrow \intersect(\p(A)[i],\p(B)[i])$
%  \ENDFOR
%  \RETURN {$typename(A)\{pa...\}$}
% \ELSE
%  \STATE $sup \leftarrow \intersect(\super(A),B)$
%  \IF {$sup = \bot$}
%   \STATE $sup \leftarrow \intersect(A,\super(B))$
%   \IF {$sup = \bot$}
%    \RETURN $\bot$
%   \ELSE
%    \STATE $sub \leftarrow B$
%   \ENDIF
%  \ELSE
%   \STATE $sub \leftarrow A$
%  \ENDIF
%  \STATE $E \leftarrow \conform(sup, \superdecl(sub))$
%  \IF {$E$ contains parameters not in $\formals(sub)$}
%   \RETURN $\bot$
%  \ENDIF
%  \RETURN $\intersect(sub, typename(sub)\{E...\})$
% \ENDIF
% %\FORALL {$(S=U) \in E$}
% % \STATE {$V \leftarrow $ value of $S$ in $sub$}
% % \STATE {replace $U$ with $intersect(U,V)$}
% %\ENDFOR
% %\RETURN {instantiate $sub$ with $E$}
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Solve type variable constraints}
% \label{alg5}
% \begin{algorithmic}
% \REQUIRE environment $X$ of pairs $T\leq S$ and $T=S$
% \ENSURE environment $Y$ of unique variable assignments $T=S$, or failure
% \STATE $Y \leftarrow \emptyset$
% \STATE replace $(T\leq S) \in X$ with $(T=S)$ when $S$ is concrete
% \FORALL {$(T=S) \in X$}
%  \IF {$(T=R) \in X$ \AND $S\neq R$}
%   \RETURN failure
%  \ENDIF
% \ENDFOR
% \FORALL {$(T\leq S) \in X$}
%  \IF {$(T=U) \in X$}
%   \IF {\NOT $\find(X,U)\leq S$}
%    \RETURN failure
%   \ELSE
%    \STATE $X \leftarrow X - (T\leq S)$
%   \ENDIF
%  \ELSIF {$(T\leq U) \in X$ and $U$ not a variable}
%   \STATE replace $U$ with $U\sqcap^{*}S$
%   \STATE $X \leftarrow X - (T\leq S)$
%  \ENDIF
% \ENDFOR
% \FORALL {variables $T$}
%  \IF {$(T=U) \in X$}
%   \STATE $Y \leftarrow Y \cup \{T=U\}$
%  \ELSE
%   \STATE $S \leftarrow \bigsqcap^{*}_{} \find(X,U), \forall (T\leq U) \in X$
%   \IF {$S = \bot$}
%    \RETURN failure
%   \ENDIF
%   \STATE $Y \leftarrow Y \cup \{T=S\}$
%  \ENDIF
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}

When the argument types are not from the same family, we recur up the
type hierarchy to see if any supertype of one of the arguments matches
the other. If so, the recursion gives us the intersected supertype $sup$,
and we face the problem of mapping it to the family of the original argument
type. To do this, we first call subroutine $conform$, which takes two types
with the same structure and returns an environment $E$ mapping any
type variables in one to their corresponding components in the other.
$superdecl(t)$ returns the type template used by $t$ to instantiate its
supertype. If all goes well, this tells us what parameters $sub$ would
have to be instantiated with to have supertype $sup$. If, however, $E$
contains type variables not controlled by $sub$, then there is no way
a type like $sub$ could have the required supertype, and the overall answer
is $\bot$.
Finally, we apply the base case to intersect $sub$ with the type obtained
by instantiating its family with parameter values in $E$.

Constraints $T\leq S$ where $S$ is a concrete type are converted to
$T=S$ to help sharpen the result type.
If Algorithm~\ref{alg5} identifies any conflicting constraints, the
type intersubsection is empty. If each type variable has exactly one
constraint $T=U$, we can simply substitute $find(X,U)$ for each occurrence
of $T$ in the computed type intersubsection, and we have a final answer.
$find$ works in the \emph{union-find} sense, following chains of equalities
until we hit a non-variable or an unconstrained variable. Unconstrained
type variables may be left in place.

The remaining case is type variables with multiple constraints. Finding
a satisfying assignment requires intersecting all the upper bounds for
a variable. It is here that we choose to throw in the towel and switch
to a coarser notion of intersubsection, denoted by $\sqcap^{*}$.
Intersubsection is effectively the inner loop of type inference, so in the
interest of getting a reasonable answer quickly we might pick
$X\sqcap^{*}Y=X$. A few simple heuristics might as well be added; for
example cases like two non-parameterized types where one is an immediate
subtype of the other can be supported easily.

In our implementation, type intersubsection handles most of the
complexity surrounding type variables and parametric methods. 
It is used to test applicability of parametric methods; since all
run-time argument lists are of concrete type, intersecting their types
with method signatures behaves like $subtype$, except static parameters
are also properly matched. If intersubsection returns $\bot$ or does not find
values for all static parameters for a method, the method is not applicable.
Therefore in practice we do not really have the freedom to implement
$\sqcap$ and $\sqcap^{*}$ any way that obeys our correctness property.
They must be at least as accurate as $subtype$ in the case where one
argument is concrete.


\subsubsection{Widening Operators}

Lattices used in practical program analyses often fail to obey the finite
chain condition necessary for the MFP algorithm to converge (i.e. they
are not of finite height) and ours is no exception.

Widening is applied in two places: by the join operator, and on every
recursive invocation of type inference.
When a union type becomes too large (as determined by an arbitrarily-chosen
cutoff), it is replaced with {\tt Any}. Tuple types lend themselves
to two infinite chains: one in depth ({\tt (Any,)}, {\tt ((Any,),)},
{\tt (((Any,),),)}, etc.) and one in length ({\tt (Any...,)},
{\tt (Any,Any...,)}, {\tt (Any,Any,Any...,)}, etc.). These chains are
capped at arbitrary cutoffs each time the inference process needs to
construct a tuple type.


%\subsubsection{Method Specificity Comparison}


\subsection{Code Generation and Optimization}

After type inference is complete, we annotate each expression with its
inferred type. We then run two symbolic optimization passes.
If the inferred argument types in a method call
indicate that a single method matches, we are free to inline that method.
For methods that return multiple values, inlining often yields
expressions that construct tuples and immediately take them apart. The
next optimization pass identifies these cases and removes the tuple
allocations.

The next set of optimizations is applied during code generation.
Our code generator targets the LLVM compiler framework \cite{LLVM}.
First, we examine uses of variables and assign local variables specific
scalar types where possible (LLVM uses a typed code representation).
The {\tt box} operations used to tag bit strings with types are done
lazily; they add a compile-time tag that causes generation of the
appropriate allocation code only when the value in question hits a context
that requires it (for example, assignment to an untyped data structure,
or being passed to an unknown function).

The code generator recognizes calls to key built-in and intrinsic functions,
and replaces them with efficient in-line code where possible. For example,
the {\tt is} function yields a pointer comparison, and {\tt typeof} might
yield a constant pointer value if the type of its argument is known.
Calls known to match single methods generate code to call the correct
method directly, skipping the dispatch process.

Finally, we run several of the optimization passes provided by LLVM.
This gives us all of the standard scalar optimizations, such as
strength reduction, dead code elimination, jump threading, and constant
folding. When
we are able to generate well-typed but messy code, LLVM gets us the
rest of the way to competitive performance. We have found that care
is needed in benchmarking: if the value computed by a loop is not
used, in some cases LLVM has been able to delete the whole thing.

%compile-time method lookup
%ccall intrinsic

% todo ?
\subsection{Run Time System}

\begin{enumerate}
\item lazy allocation of tuple types
\item storing compiler data serialized
\end{enumerate}

\section{Standard library}

\section{Performance}


\section{Ecosystem}


% -------------------------------------------------------------


%\appendix
%\section{Appendix Title}

%This is the text of the appendix, if you need one.

%\acks

%Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}

\end{document}
